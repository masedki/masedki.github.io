---
title: "Introduction aux data sciences"
output:
  html_document:
    theme: journal
    highlight: monochrome
    css: style.css
---

# Logiciel R 


- Pour une prise en main en R, je vous conseille vivement de travailler l'ensemble des exemples et exercices traités dans le [support](http://masedki.github.io/enseignements/pdfs/Introduction_to_R.pdf), initialement rédigé par Jean-Michel Marin et 
mis à jour par les deux autres auteurs. 

- Pour ceux qui ont besoin d'indications ponctuelles, vous pouvez consulter [l'aide mémoire](http://masedki.github.io/enseignements/pdfs/RCarte_Commandes-R.pdf) de Julien Chiquet.  

- Pour gagner en autonomie en programmation, la plateforme [stackoverflow](https://stackoverflow.com/) est indispensable^[Personnellement je consulte énormément cette plateforme où je trouve des réponses à mes questions.].

- [Support](http://masedki.github.io/enseignements/pdfs/notes.pdf) (évolutif) à lire. 
- Le chapitre 5 du [livre](http://masedki.github.io/enseignements/pdfs/ISLR.pdf) est indispensable et s'applique à chaque partie
 (méthode d'apprentissage) de ce cours. 

# Introduction : régression, classification et phénomène de sur-apprentissage

- **Séance du vendredi 15/02/2019:** traiter les exemples qu'on corrigera lors de la séance de cours. [Codes R](http://masedki.github.io/enseignements/rcode/code_intro.R) issu de la première séance. Les détails de chaque instruction sont données en commentaires
précédés par un #. 

# Régression : moindres carrés pour l'estimation des paramètres et choix de modèles

- **Séance du vendredi 08/03/2019:** lire la partie régression des [notes](http://masedki.github.io/enseignements/pdfs/notes.pdf) et traiter la partie 4.2 qu'on corrigera en cours. Attention la partie régression de ces notes ne peut constituer un cours de modèle linéaire classique. Les parties 3.1 et 3.2 du [livre](http://masedki.github.io/enseignements/pdfs/ISLR.pdf) sont indispensables pour l'assimilation des fondements des modèles linéaires et en particulier la partie inférence statistique^[Pour l'inférence statistique, penser aux tests et intervalles de confiance pour les paramètres de régression.]. [Code R](http://masedki.github.io/enseignements/rcode/code_reg_ridge_lasso_enet.R) : détails des exemples traités.


# Méthodes de régularisation : ridge, lasso et elasticnet

- **Séance du vendredi 15/03/2019:** lire les parties régularisation ridge, lasso et
elasticnet. Traiter les exemples autour du jeu de données du cancer de la prostate. [Code R](http://masedki.github.io/enseignements/rcode/code_reg_ridge_lasso_enet.R): ridge, lasso et elasticnet sur le jeu de données Prostate

- **Séance du vendredi 12/04/2019:**  Méthodes de régularisation en régression logistique.  Traiter l'exemple  *Sélection de variables en régression logistique : jeu de données Breast Cancer* page 33. 

[Code R 1](http://masedki.github.io/enseignements/rcode/code_logit_enet.R): Code pour la régression logistique et régression logistique avec régularisation elasticnet. 

[Code R 2](http://masedki.github.io/enseignements/rcode/classif_image_logit_enet.R): Code pour la régression logistique avec régularisation elasticnet pour la classifiction d'images. (attention le code est très lent donc je vais déposer les résultats rapidement après calcul sur un serveur). 


# Arbres de décision : régression 
- **Séance du vendredi 19/04/2019:**  Arbres de décision unique : supports de cours disponibles [ici](http://masedki.github.io/enseignements/pdfs/arbres.pdf). Lecture de la partie 8.1 du livre *An Introduction to Statistical Learning*.


[Code R](http://masedki.github.io/enseignements/rcode/rpart_insurance.R): Code pour la régression par arbre de décision avant et après élagage et comparaison avec le régression avec régularisation elasticnet. Attention : présence de données catégorielles donc traitement particulier pour l'elasticnet (voir lignes 40 et 41 du code). Lab de la partie 8.3.1 du livre *An Introduction to Statistical Learning*.


# Arbres de décision : classification 

- **Séance du vendredi 10/05/2019:** classification par arbre de décision. Supports de cours disponibles [ici](http://masedki.github.io/enseignements/pdfs/arbres.pdf). Lecture de la partie 8.1 du livre *An Introduction to Statistical Learning*.

[Code R](http://masedki.github.io/enseignements/rcode/rpart_pima.R): Code pour la classification par arbre de décision avant et après élagage et comparaison avec le régression logistique avec régularisation elasticnet. Lab de la partie 8.3.2 du livre *An Introduction to Statistical Learning*.
 


# Projet  : Utilisation de la librairie caret (sixième partie des notes)  

À rendre au plus tard le dimanche 2 juin à 23h par mail avec un fichier .R ou .rmd contenant le code et les réponses en commentaire. Mettre les affichages de vos résultats en commentaire
dans votre script. Les affichages des résultats me permettront de comprendre les conclusions et les réponses.  

<!-- [Jeu de données mnist](http://masedki.github.io/enseignements/datasets/mnist.rdata) -->

<!-- [Jeu de données mnist aplati !](http://masedki.github.io/enseignements/datasets/mnist_flatten.rda) -->

[Jeu de données coûts de soins](http://masedki.github.io/enseignements/datasets/insurance.rda)

<!-- [Jeu de données crédit](http://masedki.github.io/enseignements/datasets/credit.rda) -->

<!-- [Jeu de données spams](http://masedki.github.io/enseignements/datasets/dataspam.rda) -->

<!-- [Projet pour le parcours MSP/ES](http://masedki.github.io/enseignements/pdfs/mspes.pdf) -->