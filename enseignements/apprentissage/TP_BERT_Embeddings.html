<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ab0526a50e494ef9ba6f81e3c9b3defd</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="78HE8FLsKN9Q">
<p>Dans ce TP, nous allons passer en revue les différentes étapes pour
faire un word embeddings à l'aide de BERT de Google.</p>
</div>
<section id="introduction" class="cell markdown" id="dYapTjoYa0kO">
<h1>Introduction</h1>
</section>
<section id="un-peu-dhistoire" class="cell markdown" id="c8HDKzBai5dL">
<h3>Un peu d'histoire</h3>
<p>L'année 2018 a été une année de percée dans le domaine du NLP.
L'apprentissage par transfert, en particulier des modèles comme ELMO
d'Allen AI, Open-GPT d'OpenAI et BERT de Google, a permis aux chercheurs
de pulvériser de nombreux benchmarks avec un ajustement minimal
spécifique à la tâche et a fourni au reste de la communauté du NLP des
modèles pré-entraînés qui pourraient facilement (avec moins de données
et moins de temps de calcul) être affinés et mis en œuvre pour produire
des résultats à la pointe de la technologie. Malheureusement, la théorie
et l'application pratique de ces modèles puissants sont encore mal
comprises par les personnes qui débutent dans le domaine du NLP et même
par certains praticiens expérimentés</p>
</section>
<section id="cest-quoi-bert-" class="cell markdown" id="WoitNQMWA1bt">
<h3>C'est quoi BERT ?</h3>
<p>BERT (Bidirectional Encoder Representations from Transformers),
publié fin 2018, est le modèle que nous utiliserons dans ce TP
d'introduction à l'utilisation des modèles d'apprentissage par transfert
en NLP. BERT est une architecture neuronale pré-entraînée des
représentations du langage qui a été utilisée pour créer des modèles que
les praticiens du NLP peuvent ensuite télécharger et utiliser (quand
c'est open source). Vous pouvez utiliser ces modèles pour extraire des
caractéristiques linguistiques de haute qualité à partir de vos données
textuelles, ou vous pouvez faire une fine-tuning de ces modèles sur une
tâche spécifique (classification, extractions d'information,réponse à
des questions, etc.) avec vos propres données.</p>
</section>
<section id="pourquoi-un-embeddings-via-bert-" class="cell markdown"
id="q-dDVmXAA3At">
<h3>Pourquoi un embeddings via BERT ?</h3>
<p>Dans ce tutoriel, nous utiliserons BERT pour extraire des features, à
savoir des vecteurs d'embeddings de mots et de phrases, à partir de
données textuelles. Que pouvons-nous faire avec ces vecteurs
d'embeddings de mots et de phrases ? On peut les utiliser pout toute
tâche d'apprentissage.</p>
<p>Deuxièmement, et c'est peut-être le point le plus important, ces
vecteurs sont utilisés comme entrées (vecteur de variables explicatives
) dans des modèles d'apprentissage. Dans le passé, les mots ont été
représentés par codage disjonctif complet (one-hot encoding), soit, de
manière plus utile, comme embeddings de mots par modèles neuronaux tels
que Word2Vec ou Fasttext. BERT offre un avantage par rapport à des
modèles comme Word2Vec, car alors que chaque mot a une représentation
fixe dans Word2Vec, quel que soit le contexte dans lequel il apparaît,
BERT produit des représentations de mots qui sont dynamiquement
informées par les mots qui les entourent. Par exemple, si l'on considère
deux phrases :</p>
<ul>
<li><p>"The man was accused of robbing a bank."</p></li>
<li><p>"The man went fishing by the bank of the river."</p></li>
</ul>
<p>Word2Vec would produce the same word embedding for the word "bank" in
both sentences, while under BERT the word embedding for "bank" would be
different for each sentence. Aside from capturing obvious differences
like polysemy, the context-informed word embeddings capture other forms
of information that result in more accurate feature representations,
which in turn results in better model performance.</p>
<p>Word2Vec produirait le même embedding pour le mot « banque » dans les
deux phrases, alors qu'avec BERT, l'embedding pour « banque » serait
différent pour chaque phrase. Outre la prise en compte de différences
évidentes telles que la polysémie, l'embedding des mots en fonction du
contexte permet de saisir d'autres formes d'informations qui se
traduisent par des représentations plus précises, ce qui se traduit par
une meilleure performance des modèles d'apprentissage.</p>
<p>From an educational standpoint, a close examination of BERT word
embeddings is a good way to get your feet wet with BERT and its family
of transfer learning models, and sets us up with some practical
knowledge and context to better understand the inner details of the
model in later tutorials.</p>
<p>D'un point de vue pédagogique, un examen approfondi des embeddings de
BERT est un bon moyen de se familiariser avec BERT et son utilisation
pour l'apprentissage par transfert.</p>
</section>
<section id="1-charger-le-modèle-pré-entraîné-bert"
class="cell markdown" id="Pqa-7WXBAw8q">
<h1>1. Charger le modèle pré-entraîné BERT</h1>
</section>
<div class="cell markdown" id="eCdqJCtQN52l">
<p>Installation de l'interface pytorch pour BERT via le dépôt Hugging
Face. (Cette bibliothèque contient des interfaces pour d'autres modèles
de langage pré-entraînés comme GPT et GPT-2 d'OpenAI).</p>
<p>Sur Google Colab, on doit installer cette bibliothèque à chaque
reconnection.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:9916,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568336911,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="1RfUN_KolV-f" data-outputId="c9a8a10f-18f5-4e60-8209-57f2c0a821f0">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install transformers</span></code></pre></div>
</div>
<div class="cell markdown" id="JSXImOxMPdNg">
<p>Nous allons importer pytorch, le modèle pré-entraîné BERT et son
tokenizer.</p>
<p>Nous n'allons pas expliquer le modèle en détail dans ce tp. Il s'agit
du modèle pré-entraîné publié par Google qui a fonctionné pendant de
très nombreuses heures sur Wikipédia et <a
href="https://arxiv.org/pdf/1506.06724.pdf">Book Corpus</a>, un ensemble
de données contenant +10 000 livres de différents genres. Ce modèle est
responsable (avec quelques modifications) d'avoir battu des benchmarks
NLP dans un grand nombre de tâches. Google a publié quelques variantes
des modèles BERT, mais celui que nous utiliserons ici est le plus petit
des deux tailles disponibles (« base » et « large ») et ignore
insensible à la casse (insensible aux majuscules et minuscules), d'où le
terme « uncased ».</p>
<p><code>transformers</code> fournit un certain nombre de classes pour
appliquer BERT à différentes tâches (classification de tokens (jetons),
classification de textes, ...). Ici, nous utilisons le
<code>BertModel</code> de base qui n'a pas de tâche de sortie spécifique
- c'est un bon choix pour utiliser BERT juste pour extraire des
embeddings.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:249,&quot;referenced_widgets&quot;:[&quot;82e752d640e84e028e694670313acd77&quot;,&quot;54e497f0df8944eaaf24f5d8f6a8bdfe&quot;,&quot;a31d68caa1ae49e7ba881673b9bbe020&quot;,&quot;16f9ad65f40c44dfab098829af3a4f56&quot;,&quot;54eb6958caae452e8fa31f573d16d396&quot;,&quot;3b7fe5ce602b4ecc994681924ac2be28&quot;,&quot;b2c4eaaa09c74c6b87f50ec0b807f0db&quot;,&quot;9812da1ad6ff47a083ee67e1b4ddc934&quot;,&quot;4ce3e98f85f948608ce9f5bc2c12cccb&quot;,&quot;4c05a8e43e1e4a2fb1ef82634ed89489&quot;,&quot;7a3b8b206a3640e3a1e837ad530e07b0&quot;,&quot;0dc65e8565df46c4938c96815e1c6dfa&quot;,&quot;40dc31e51ad246e1b4d7603e24b5547a&quot;,&quot;970cac3a8e7a4c9592817dec75a8c7c2&quot;,&quot;e399bdb344e84a7888c6573e1ab9b072&quot;,&quot;7f575fb0190c423c889e99f26c3de4ed&quot;,&quot;418ba4f470e4418bbbb0fafd1c85cbf9&quot;,&quot;ab9c28477429471a91d318e2d8299d4b&quot;,&quot;f96caa9b3b7649ea82ec64c619f44361&quot;,&quot;ca2a0bfbda2f400bbd6f0c2b104c4aec&quot;,&quot;133ba015ef2640afb7e7ed3cdacabe80&quot;,&quot;f933890a8c934c24987d8b1f83358af6&quot;,&quot;3872617fe9be4244a04fd1ecf4c337a0&quot;,&quot;749ea54bb74446bb92ba327d5f5e3097&quot;,&quot;3010abdd61fa472b82425388f46f4e55&quot;,&quot;b42a0196e42e4ad8b8aaf9a11e1cc48d&quot;,&quot;22c361e7df82498c9b674788cad80d4b&quot;,&quot;ec99b96c0d0e43558eaa1d575254921e&quot;,&quot;bd96dac03dd34061b545d3bc72880604&quot;,&quot;cc131ab1e8194f9cb0b759285648280c&quot;,&quot;547999105b7e4e19b794433de698aa61&quot;,&quot;fef9e32818eb43e6b01ba855e178c822&quot;,&quot;c1992eb9b14a44cc9546379237ca2b21&quot;,&quot;5f69cde0db214cb1a27f6d8eb94bbf2e&quot;,&quot;17e266f69c0f43d4a09b24605fc80edc&quot;,&quot;2cc547c96f71478988108f7a7b54ee13&quot;,&quot;a3c68c5bfc4840c0934707f502c606fd&quot;,&quot;06c2507a72b4460993ffcbf3557d5380&quot;,&quot;e4366ede88b749e9b5bb4259de636757&quot;,&quot;e470b015d95642c5b114345a4575237c&quot;,&quot;b39a99dc381f44e4ab2a9fd3c0d11e6f&quot;,&quot;e7126aad1ca54f8082ff0d247ee88f68&quot;,&quot;42993d2dc22048588c96214be6a3f1e3&quot;,&quot;b98bf686ebd54932bfbdaad268254a7e&quot;]}"
data-executionInfo="{&quot;elapsed&quot;:31912,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368820,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="lJEnBJ3gHTsQ" data-outputId="acc8097f-3ba1-48d3-9428-a7f26c14d2ce">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># OPTIONAL: if you want to have more information on what&#39;s happening, activate the logger as follows</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#logging.basicConfig(level=logging.INFO)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained model tokenizer (vocabulary)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>)</span></code></pre></div>
</div>
<section id="2-le-bon-format-de-lentrée" class="cell markdown"
id="Tlv3VlPnKKHN">
<h1>2. Le bon format de l'entrée</h1>
<p>Because BERT is a pretrained model that expects input data in a
specific format, we will need: Comme BERT est pré-entraîné, il nous
impose un format d'entrée des données spécifique.</p>
<ol>
<li><p>Un <strong>special token, <code>[SEP]</code>,</strong> pour
désigner la fin d'une phrase ou la séparation entre deux
phrases.</p></li>
<li><p>Un <strong>special token, <code>[CLS]</code>,</strong> au début
du texte, il est utilisé pour des tâches de classification, mais BERT
l'utilise aussi pour désigner le début du texte indépendamment de la
tâche à mener.</p></li>
<li><p>Un découpage en Tokens (jetons) conforme au vocabulaire fixé dans
BERT</p></li>
<li><p>Les <strong>Token IDs</strong> qui sont stockés dans le tokenizer
de BERT</p></li>
<li><p>Les <strong>Mask IDs</strong> pour indiquer les éléments d'une
sequence qui sont des tokens et ceux qui sont des éléments de complétion
(padding elements)</p></li>
<li><p><strong>Segment IDs</strong> pour désigner les différents phrases
d'un texte</p></li>
<li><p><strong>Positional Embeddings</strong> (qui demande plus de
travail pour l'expliquer) utilisé pour informer de la position du token
dans une phrase (ou séquence de mots)</p></li>
</ol>
<p>Bonne nouvelle, la librairie <code>transformers</code> prend en
charge tout ce cahier de charges (en utilisant la fonction
<code>tokenizer.encode_plus</code>).</p>
<p>Comme il s'agit d'une introduction à l'utilisation de BERT, nous
allons effectuer ces étapes de manière (principalement) manuelle.</p>
</section>
<section id="21-jetons-spéciaux-special-tokens" class="cell markdown"
id="diVtyCJCurxJ">
<h2>2.1. Jetons spéciaux (special Tokens)</h2>
<p>BERT peut prendre en entrée une ou deux phrases, et utilise le token
spécial <code>[SEP]</code> pour les différencier. Le token
<code>[CLS]</code> apparaît toujours au début du texte et est spécifique
aux tâches de classification.</p>
<p>Les deux tokens sont <em>toujours requis</em>, cependant, même si
nous n'avons qu'une seule phrase, et même si nous n'utilisons pas
l'outil BERT pour la classification. C'est ainsi que BERT a été
pré-entraîné, et c'est donc ce que BERT s'attend à voir.</p>
<p><strong>2 phrases en entrée</strong>:</p>
<p><code>[CLS] The man went to the store. [SEP] He bought a gallon of milk.</code></p>
<p><strong>1 phrase en entrée</strong>:</p>
<p><code>[CLS] The man went to the store. [SEP]</code></p>
</section>
<section id="22-opération-de-tokenization-découpage-en-jetons-de-base"
class="cell markdown" id="3gsyrAwYvBfC">
<h2>2.2. Opération de "Tokenization" (découpage en jetons de base)</h2>
</section>
<div class="cell markdown" id="2WafgQPLAWmo">
<p>BERT fournit son propre tokenizer, que nous avons importé ci-dessus.
Voyons comment il traite la phrase ci-dessous.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:16,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368821,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="Pg0P9rFxJwwp" data-outputId="bde3af57-eefa-48bf-b790-c9cf371a7773">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot;Here is the sentence I want embeddings for.&quot;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>marked_text <span class="op">=</span> <span class="st">&quot;[CLS] &quot;</span> <span class="op">+</span> text <span class="op">+</span> <span class="st">&quot; [SEP]&quot;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokeniser la phrase à l&#39;aide du tokenizer de BERT.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tokenized_text <span class="op">=</span> tokenizer.tokenize(marked_text)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Affichage des jetons.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (tokenized_text)</span></code></pre></div>
</div>
<div class="cell markdown" id="Q51eN4KAkbIJ">
<p>Le mot « embeddings » est représenté (ou découpé en jetons) comme
suit:</p>
<p><code>['em', '##bed', '##ding', '##s']</code></p>
<p>Le mot original a été divisé en sous-mots et caractères plus petits.
Les deux signes ## qui précèdent certains de ces sous-mots sont
simplement la façon dont notre tokenizer indique que ce sous-mot ou ce
caractère fait partie d'un mot plus large et qu'il est précédé d'un
autre sous-mot. Ainsi, par exemple, le jeton « ##bed » est distinct du
jeton « bed » ; le premier est utilisé chaque fois que le sous-mot « bed
» apparaît dans un mot plus large et le second est utilisé explicitement
lorsque le jeton correspondant au mot «bed (lit)» apparaît.</p>
<p>Why does it look this way? This is because the BERT tokenizer was
created with a WordPiece model. This model greedily creates a fixed-size
vocabulary of individual characters, subwords, and words that best fits
our language data. Since the vocabulary limit size of our BERT tokenizer
model is 30,000, the WordPiece model generated a vocabulary that
contains all English characters plus the ~30,000 most common words and
subwords found in the English language corpus the model is trained on.
This vocabulary contains four things:</p>
<p>de quoi s'agit-il ? C'est parce que le tokenizer de BERT a été créé
avec un modèle WordPiece. Ce modèle crée un vocabulaire de taille fixe
composé de caractères individuels, de sous-mots et de mots qui
correspondent le mieux à nos données linguistiques. Étant donné que la
taille limite du vocabulaire de notre tokenizer BERT est de 30 000, le
modèle WordPiece a généré un vocabulaire qui contient tous les
caractères anglais ainsi que les ~30 000 mots et sous-mots les plus
courants trouvés dans le corpus de langue anglaise sur lequel le modèle
est entraîné. Ce vocabulaire contient quatre éléments :</p>
<ol>
<li><p>Mots entiers</p></li>
<li><p>Sous-mots apparaissant au début d'un mot ou isolément (« em »
comme dans « embeddings » se voit attribuer le même vecteur que la
séquence autonome de caractères « em » comme dans « go get em »
)</p></li>
<li><p>Les sous-mots ne se trouvant pas au début d'un mot, qui sont
précédés de « ## » pour indiquer ce cas.</p></li>
<li><p>Caractères individuels</p></li>
</ol>
<p>Pour tokeniser un mot dans le cadre de ce modèle, le tokenizer
vérifie d'abord si le mot entier figure dans le vocabulaire. Si ce n'est
pas le cas, il tente de décomposer le mot en sous-mots les plus larges
possibles contenus dans le vocabulaire et, en dernier recours, il
décompose le mot en caractères individuels. Notez que, pour cette
raison, nous pouvons toujours représenter un mot comme étant, au
minimum, la collection de ses caractères individuels.</p>
<p>Par conséquent, plutôt que d'assigner les mots hors vocabulaire à un
jeton fourre-tout tel que « OOV » ou « UNK », les mots qui ne font pas
partie du vocabulaire sont décomposés en sous-mots et en tokens de
caractères pour lesquels nous pouvons ensuite générer des
embeddings.</p>
<p>Ainsi, plutôt que d'attribuer « embeddings » et tout autre mot hors
vocabulaire à un jeton de vocabulaire inconnu surchargé, nous le
divisons en sous-mots ['em', '##bed', '##ding', '##s'] qui conserveront
une partie de la signification contextuelle du mot d'origine. Nous
pouvons même faire la moyenne de ces vecteurs d'embedding de sous-mots
pour générer un vecteur approximatif pour le mot d'origine.</p>
<p>(Pour plus d'information sur WordPiece, voir<a
href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf">original
paper</a> et échanges google <a
href="https://arxiv.org/pdf/1609.08144.pdf">Neural Machine Translation
System</a>.)</p>
</div>
<div class="cell markdown" id="jp5zXAPBVp82">
<p>Voici quelques exemples de tokens du vocabulaire. Les tokens
commençant par ## sont des sous-mots ou des caractères individuels.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:15,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368821,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="1z1SzuTrqx-7" data-outputId="d6fab7e6-3ea1-4815-c55a-d0e78f1dbd56">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(tokenizer.vocab.keys())[<span class="dv">5000</span>:<span class="dv">5020</span>]</span></code></pre></div>
</div>
<div class="cell markdown" id="HoF3LC47VgBb">
<p>Après avoir divisé le texte en jetons, nous devons convertir la
phrase d'une liste de chaînes de caractères en une liste d'indices de
vocabulaire.</p>
<p>À partir de là, nous utiliserons l'exemple de phrase ci-dessous, qui
contient deux occurrences du mot «bank» avec des significations
différentes.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:12,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368821,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="XYjcYJuXoAQx" data-outputId="e681836e-c2d9-44a1-a3c6-42705b5aff08">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Un autre exemple de phrase avec différentes significations du mot &quot;bank&quot;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot;After stealing money from the bank vault, the bank robber was seen &quot;</span> \</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>       <span class="st">&quot;fishing on the Mississippi river bank.&quot;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajouter les deux tokens spéciaux</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>marked_text <span class="op">=</span> <span class="st">&quot;[CLS] &quot;</span> <span class="op">+</span> text <span class="op">+</span> <span class="st">&quot; [SEP]&quot;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Diviser la phrase en tokens.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>tokenized_text <span class="op">=</span> tokenizer.tokenize(marked_text)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Récupérer les indices des tokens dans le vocabulaire.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>indexed_tokens <span class="op">=</span> tokenizer.convert_tokens_to_ids(tokenized_text)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Affichage des mots avec les indices.</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tup <span class="kw">in</span> <span class="bu">zip</span>(tokenized_text, indexed_tokens):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="sc">{:&lt;12}</span><span class="st"> {:&gt;6,}&#39;</span>.<span class="bu">format</span>(tup[<span class="dv">0</span>], tup[<span class="dv">1</span>]))</span></code></pre></div>
</div>
<section id="23-identification-dune-phrase-segment-id"
class="cell markdown" id="if6C_iCULU60">
<h2>2.3. Identification d'une phrase (segment ID)</h2>
<p>Pour chaque token dans "tokenized_text", nous devons spécifier à
quelle phrase il appartient : la phrase 0 (une série de 0) ou la phrase
1 (une série de 1). Pour nos besoins, les entrées de phrases simples ne
nécessitent qu'une série de 1, nous créerons donc un vecteur de 1 pour
chaque token de notre phrase d'entrée.</p>
<p>Si on veut traiter deux phrases, attribuez un 0 à chaque mot de la
première phrase ainsi qu'au jeton « [SEP] », et un 1 à tous les jetons
de la deuxième phrase.</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368821,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="u_jEkVKxJMc0" data-outputId="5f0f6cf0-5819-486f-fe19-72528f872793">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Marquer chacun des 22 tokens comme appartenant à la phrase &quot;1&quot;.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>segments_ids <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(tokenized_text)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(segments_ids)</span></code></pre></div>
</div>
<section id="3-récupérer-les-embeddings" class="cell markdown"
id="c-nY9LASLr2L">
<h1>3. Récupérer les embeddings</h1>
</section>
<section id="31-faire-tourner-bert-sur-nos-bouts-de-textes"
class="cell markdown" id="sl-iCj8wMEd5">
<h2>3.1. Faire tourner BERT sur nos bouts de textes</h2>
</section>
<div class="cell markdown" id="_Nvaw46mfc8M">
<p>Nous allons convertir nos données en tenseurs torch et appeler le
modèle BERT. L'interface BERT PyTorch exige que les données soient dans
des tenseurs torch plutôt que dans des listes Python, nous convertissons
donc les listes ici - cela ne change pas la forme ou les données.</p>
</div>
<div class="cell code"
data-executionInfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568368821,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="E_t4cM6KLc98">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convertir en PyTorch tensors</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tokens_tensor <span class="op">=</span> torch.tensor([indexed_tokens])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>segments_tensors <span class="op">=</span> torch.tensor([segments_ids])</span></code></pre></div>
</div>
<div class="cell markdown" id="UCIGe0AXfg4Z">
<p>L'appel à <code>from_pretrained</code> va récupérer le modèle.
Lorsque nous chargeons <code>bert-base-uncased</code>, nous voyons la
définition du modèle affichée. Le modèle est un réseau neuronal profond
avec 12 couches ! L'explication des couches et de leurs fonctions
n'entre pas dans le cadre de ce TP.</p>
<p>model.eval() place notre modèle en mode évaluation par opposition au
mode entraînement. Dans ce cas, le mode évaluation désactive la
régularisation dropout qui est utilisée lors de l'apprentissage.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:761,&quot;referenced_widgets&quot;:[&quot;39106354366148978c64d21fc9bdb14c&quot;,&quot;1089d8a30f574eeea240b6bbf4bf79d6&quot;,&quot;04086a1008b140cc843eca2163e02dc2&quot;,&quot;6a43d6b5de0d4cf2aba4bddb6bf59f01&quot;,&quot;3c81993c04174f4aadbea1065371c487&quot;,&quot;be0f5640dbd04f16af95fd09abf39b6c&quot;,&quot;8398c65c9d344fdcb779849021b7774d&quot;,&quot;86c3923bfab5464aa1d2ef0b69258293&quot;,&quot;23ef405843dc4734853f8a0c335ca9b2&quot;,&quot;0fdbeaf97dc449b5a562700eb4c9e60f&quot;,&quot;0fe4e76f38484425bd44695d181b3abc&quot;]}"
data-executionInfo="{&quot;elapsed&quot;:5119,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568373933,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="Mq2PKplWfbFv" data-outputId="80f0e302-caa0-4239-ecfa-1e5a8caf977f">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># charger le modèle pré-entraîné (les poids)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                                  output_hidden_states <span class="op">=</span> <span class="va">True</span>, <span class="co"># Si le modèle renvoie tous les états cachés.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>                                  )</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Met le modèle en mode &quot;evaluation&quot;,  pour faire un passe-avant.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span></code></pre></div>
</div>
<div class="cell markdown" id="G4Qa5KkkM2Aq">
<p>Evaluons BERT sur notre exemple, et récupérons les états cachés du
réseau !</p>
<p><em><code>torch.no_grad</code> indique à PyTorch de ne pas construire
le graphe de calcul du gradient pendant cette passe-avant (puisque nous
n'exécuterons pas de rétro-prop ici) -- cela réduit simplement la
consommation de mémoire et accélère un peu les choses.</em></p>
</div>
<div class="cell code"
data-executionInfo="{&quot;elapsed&quot;:616,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568374546,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="nN0QTZwiMzeq">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Passer le texte dans le réseau de neurones BERT, et collecter tous les états cachés</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># des 12 couches.</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(tokens_tensor, segments_tensors)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluating the model will return a different number of objects based on</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># how it&#39;s  configured in the `from_pretrained` call earlier. In this case,</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># becase we set `output_hidden_states = True`, the third item will be the</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hidden states from all layers. See the documentation for more details:</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://huggingface.co/transformers/model_doc/bert.html#bertmodel</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> outputs[<span class="dv">2</span>]</span></code></pre></div>
</div>
<section id="32-comprendre-la-sortie-de-la-passe-avant"
class="cell markdown" id="UeQNEFbUgMSf">
<h2>3.2. Comprendre la sortie de la passe-avant</h2>
</section>
<div class="cell markdown" id="HKTlTS_sfuAe">
<p>L'ensemble des états cachés de ce modèle, stockés dans l'objet
<code>hidden_states</code>, est un peu vertigineux. Cet objet a quatre
dimensions, dans l'ordre suivant</p>
<ol>
<li>Le numéro de la couche (13 couches)</li>
<li>Le numéro de lot (1 phrase)</li>
<li>Le nombre de mots / tokens (22 tokens dans notre phrase)</li>
<li>L'unité cachée / la dimension de l'embeddings (768 features)</li>
</ol>
<p>13 couches alors que BERT n'en a pas seulement 12 ? C'est 13 parce
que le premier élément est l'embedding d'entrée, le reste étant les
sorties de chacune des 12 couches de BERT.</p>
<p>Bilan : 219648 valeurs uniques pour représenter notre phrase !</p>
<p>La deuxième dimension, la taille du lot, est utilisée lorsque
plusieurs phrases sont soumises au modèle en même temps ; ici, nous
n'avons qu'une seule phrase d'exemple.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:6,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568374546,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="eI_uxiW7eRWA" data-outputId="65cf6e4a-a1f2-431c-e329-95e1290cde76">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;Nombre de couches :&quot;</span>, <span class="bu">len</span>(hidden_states), <span class="st">&quot;  (embeddings initial + 12 couches de BERT)&quot;</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>layer_i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;Nombre de lots:&quot;</span>, <span class="bu">len</span>(hidden_states[layer_i]))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>batch_i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;Nombre de tokens:&quot;</span>, <span class="bu">len</span>(hidden_states[layer_i][batch_i]))</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>token_i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;Nombre d&#39;unités cachées:&quot;</span>, <span class="bu">len</span>(hidden_states[layer_i][batch_i][token_i]))</span></code></pre></div>
</div>
<div class="cell markdown" id="6Uc_S_hmOWe7">
<p>On remarque que les valeurs renvoyées par toutes les couches et tous
les jetons, la majorité des valeurs se situant entre [-2, 2] et un petit
nombre de valeurs autour de -10.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:830}"
data-executionInfo="{&quot;elapsed&quot;:1338,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375880,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="-UF_OAO-S1sP" data-outputId="7679dfa7-0192-408e-cd72-f0fca739ed32">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pout le token 5 dans notre phrase, regarder les données de la couche 5.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>token_i <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>layer_i <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> hidden_states[layer_i][batch_i][token_i]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the values as a histogram to show their distribution.</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.hist(vec, bins<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="cell markdown" id="n194RcReDYfw">
<p>Le regroupement des valeurs par couche est logique pour le modèle,
mais pour nos besoins, nous voulons qu'elles soient regroupées par
token.</p>
<p>Dimensions actuelles :</p>
<p><code>[# layers, # batches, # tokens, # features]</code></p>
<p>Dimensions souhaitées :</p>
<p><code>[# tokens, # layers, # features]</code></p>
<p>PyTorch offre la fonction <code>permute</code> pour réarranger
facilement les dimensions d'un tenseur.</p>
<p>Cependant, la première dimension est actuellement une liste Python
!</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:12,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375880,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="0CcY_oRwcHlS" data-outputId="8f365354-488d-407c-bbea-81c11caa1b59">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># `hidden_states` est une liste Python.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;      Type of hidden_states: &#39;</span>, <span class="bu">type</span>(hidden_states))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># chaque couche dans la liste est un tenseur torch.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Taille du tenseur de chaque couche: &#39;</span>, hidden_states[<span class="dv">0</span>].size())</span></code></pre></div>
</div>
<div class="cell markdown" id="1yXZjLSke3F0">
<p>Combinons les couches pour obtenir un grand tenseur.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:12,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375881,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="pTJV8AFFcLbL" data-outputId="513eee28-ab5f-435d-934d-27801ce2596b">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># on regroupe les tenseurs pour toutes les couches. On utilise la méthode `stack`</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ça donne de nouvelles dimensions de tenseur.</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> torch.stack(hidden_states, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>token_embeddings.size()</span></code></pre></div>
</div>
<div class="cell markdown" id="rnBv2TUNhzf4">
<p>Supprimons la dimension "lots" puisque nous n'en avons pas
besoin.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:11,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375881,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="En4JZ41fh6CI" data-outputId="974b78c8-2561-40c0-92f1-d8c1ecb09a29">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Supprimer la dimension 1 des &quot;batches&quot;.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> torch.squeeze(token_embeddings, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>token_embeddings.size()</span></code></pre></div>
</div>
<div class="cell markdown" id="YVzRfvkbe-Yp">
<p>Enfin, nous pouvons alterner les dimensions "layers" et "tokens" avec
<code>permute</code>.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375881,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="AtDVE58cdeYp" data-outputId="36941ab4-3ff6-40dd-e0c1-8a1376fdfeba">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># alterner les dimensions</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> token_embeddings.permute(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>token_embeddings.size()</span></code></pre></div>
</div>
<section
id="33-création-de-vecteurs-de-représentation-des-mots-et-phrases-à-partir-des-hidden-states"
class="cell markdown" id="Ey5RhOQ7NGtz">
<h2>3.3. Création de vecteurs de représentation des mots et phrases à
partir des hidden states</h2>
<p>Maintenant, que faisons-nous avec ces états cachés ? Nous aimerions
obtenir des vecteurs individuels pour chacun de nos tokens, ou peut-être
une représentation vectorielle unique de la phrase entière, mais pour
chaque token de notre entrée, nous disposons de 13 vecteurs distincts,
chacun d'une longueur de 768.</p>
<p>Pour obtenir les vecteurs individuels, nous devons combiner certains
des vecteurs des couches... mais quelle couche ou combinaison de couches
fournit la meilleure représentation ?</p>
<p>Malheureusement, il n'y a pas de réponse simple et unique... Essayons
quelques approches raisonnables.</p>
</section>
<section id="les-word-vectors" class="cell markdown" id="76TdtFH8NM9q">
<h3>Les <em>Word Vectors</em></h3>
<p>Pour se donner quelques exemples, testons deux approches.</p>
<p>Premièrement, appliquer un <strong>concatenate</strong> les 4
dernières couches, nous donnent un seul vecteur par token. Chaque
vecteur sera de taille <code>4 x 768 = 3072</code>.</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:9,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375881,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="pv42h9jANMRf" data-outputId="8d07a709-cb9e-4a8e-9a5c-13a81c7801c2">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Stores the token vectors, with shape [22 x 3,072]</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>token_vecs_cat <span class="op">=</span> []</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `token_embeddings` is a [22 x 12 x 768] tensor.</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For each token in the sentence...</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> token_embeddings:</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `token` is a [12 x 768] tensor</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate the vectors (that is, append them together) from the last</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># four layers.</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each layer vector is 768 values, so `cat_vec` is length 3,072.</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    cat_vec <span class="op">=</span> torch.cat((token[<span class="op">-</span><span class="dv">1</span>], token[<span class="op">-</span><span class="dv">2</span>], token[<span class="op">-</span><span class="dv">3</span>], token[<span class="op">-</span><span class="dv">4</span>]), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use `cat_vec` to represent `token`.</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    token_vecs_cat.append(cat_vec)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;Shape is: </span><span class="sc">%d</span><span class="st"> x </span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> (<span class="bu">len</span>(token_vecs_cat), <span class="bu">len</span>(token_vecs_cat[<span class="dv">0</span>])))</span></code></pre></div>
</div>
<div class="cell markdown" id="VnWaByfelM-e">
<p>Une autre méthode consiste à créer des word vectors en faisant la
somme des 4 dernières.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:8,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375881,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="j4DKDtFwiF0S" data-outputId="372ee23c-e6c9-4706-90a7-c89ea5919271">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Stores the token vectors, with shape [22 x 768]</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>token_vecs_sum <span class="op">=</span> []</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `token_embeddings` is a [22 x 12 x 768] tensor.</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For each token in the sentence...</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> token_embeddings:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `token` is a [12 x 768] tensor</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sum the vectors from the last four layers.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    sum_vec <span class="op">=</span> torch.<span class="bu">sum</span>(token[<span class="op">-</span><span class="dv">4</span>:], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use `sum_vec` to represent `token`.</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    token_vecs_sum.append(sum_vec)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;Shape is: </span><span class="sc">%d</span><span class="st"> x </span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> (<span class="bu">len</span>(token_vecs_sum), <span class="bu">len</span>(token_vecs_sum[<span class="dv">0</span>])))</span></code></pre></div>
</div>
<section id="embedding-dune-phrase" class="cell markdown"
id="mQaco6jRLkXn">
<h3>Embedding d'une phrase</h3>
</section>
<div class="cell markdown" id="uuul6iQqnXT2">
<p>Pour obtenir un vecteur unique pour l'ensemble de notre phrase, nous
disposons de plusieurs stratégies en fonction de notre objectif, mais
une approche simple consiste à faire la moyenne de l'avant-dernière
couche cachée de chaque token, ce qui produit un vecteur unique d'une
longueur de 768.</p>
</div>
<div class="cell code"
data-executionInfo="{&quot;elapsed&quot;:8,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375882,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="Zn0n2S-FWZih">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># `hidden_states` has shape [13 x 1 x 22 x 768]</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `token_vecs` is a tensor with shape [22 x 768]</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>token_vecs <span class="op">=</span> hidden_states[<span class="op">-</span><span class="dv">2</span>][<span class="dv">0</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the average of all 22 token vectors.</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>sentence_embedding <span class="op">=</span> torch.mean(token_vecs, dim<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375882,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="MQv0FL8VWadn" data-outputId="04427bc4-f86a-436e-aa78-22720c42fc2d">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;embedding de notre phrase est un vecteur de la forme:&quot;</span>, sentence_embedding.size())</span></code></pre></div>
</div>
<section id="34-confirmation-des-vecteurs-dépendant-du-contexte"
class="cell markdown" id="TqYcrAipfE3E">
<h2>3.4. Confirmation des vecteurs dépendant du contexte</h2>
<p>To confirm that the value of these vectors are in fact contextually
dependent, let's look at the different instances of the word "bank" in
our example sentence:</p>
<p>"After stealing money from the <strong>bank vault</strong>, the
<strong>bank robber</strong> was seen fishing on the Mississippi
<strong>river bank</strong>."</p>
<p>Let's find the index of those three instances of the word "bank" in
the example sentence.</p>
<p>Pour confirmer que la valeur de ces vecteurs dépend effectivement du
contexte, examinons les différentes occurrences du mot « bank » dans
notre phrase d'exemple :</p>
<p>"After stealing money from the <strong>bank vault</strong>, the
<strong>bank robber</strong> was seen fishing on the Mississippi
<strong>river bank</strong>."</p>
<p>Trouvons l'indice de ces trois occurrences du mot « bank » dans la
phrase d'exemple.</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568375883,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="DNiRsEh9cmWz" data-outputId="44a152a3-7566-4f60-d02b-12a7d317449b">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, token_str <span class="kw">in</span> <span class="bu">enumerate</span>(tokenized_text):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span> (i, token_str)</span></code></pre></div>
</div>
<div class="cell markdown" id="AEhBIA5RlS8-">
<p>3 occurrences à 6, 10, and 19.</p>
<p>Pour cette analyse, nous utiliserons les vecteurs de mots que nous
avons créés en additionnant les quatre dernières couches. Nous pouvons
essayer d'imprimer leurs vecteurs pour les comparer.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:655,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568376532,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="tBa6vRHknSkv" data-outputId="831ec364-a704-4808-a068-51cdf966e446">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Les 5 premières composantes du vecteur pour chaque occurrence de &quot;bank&quot;.&#39;</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;&#39;</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;bank vault   &quot;</span>, <span class="bu">str</span>(token_vecs_sum[<span class="dv">6</span>][:<span class="dv">5</span>]))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;bank robber  &quot;</span>, <span class="bu">str</span>(token_vecs_sum[<span class="dv">10</span>][:<span class="dv">5</span>]))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;river bank   &quot;</span>, <span class="bu">str</span>(token_vecs_sum[<span class="dv">19</span>][:<span class="dv">5</span>]))</span></code></pre></div>
</div>
<div class="cell markdown" id="Ca2TCQ_G7SM3">
<p>Nous pouvons constater que les valeurs diffèrent, mais calculons la
similarité cosinus entre les vecteurs pour effectuer une comparaison
plus précise.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-executionInfo="{&quot;elapsed&quot;:9,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1737568376532,&quot;user&quot;:{&quot;displayName&quot;:&quot;Moh. A. Sedki&quot;,&quot;userId&quot;:&quot;17778918373331917767&quot;},&quot;user_tz&quot;:-60}"
id="eYXUwiG0yhBS" data-outputId="c76ea308-e23f-426d-e5e5-17d7f3ae6516">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cosine</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;bank robber&quot; vs &quot;river bank&quot; (different meanings).</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>diff_bank <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(token_vecs_sum[<span class="dv">10</span>], token_vecs_sum[<span class="dv">19</span>])</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;bank robber&quot; vs &quot;bank vault&quot; (same meaning).</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>same_bank <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(token_vecs_sum[<span class="dv">10</span>], token_vecs_sum[<span class="dv">6</span>])</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;valeur de la similarité dans le cas (bank pour banque dans les deux cas):  </span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> same_bank)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;valeur de la similarité dans le cas (bank pour banque et rive ):  </span><span class="sc">%.2f</span><span class="st">&#39;</span> <span class="op">%</span> diff_bank)</span></code></pre></div>
</div>
<section id="35-stratégie-de-pooling-et-choix-des-couches"
class="cell markdown" id="orjhWUJgmxo5">
<h2>3.5. Stratégie de pooling et choix des couches</h2>
</section>
<div class="cell markdown" id="f1CI97kNn8dD">
<p>Ci-dessous quelques ressources supplémentaires pour explorer ce
sujet.</p>
</div>
<div class="cell markdown" id="P3D5qnRNmq5_">
<p><strong>Auteurs de BERT</strong></p>
<p>Les auteurs de l'étude BERT ont testé des stratégies d'intégration de
mots en introduisant différentes combinaisons de vecteurs comme
caractéristiques d'entrée dans une BiLSTM utilisée pour une tâche de
reconnaissance d'entités nommées et en observant les scores F1
obtenus.</p>
<p>(Image du blog <a
href="http://jalammar.github.io/illustrated-bert/">Jay Allamar</a>)</p>
<p><img
src="http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png"
alt="alt text" /></p>
<p>Bien que la concaténation des quatre dernières couches ait produit
les meilleurs résultats pour cette tâche spécifique, de nombreuses
autres méthodes arrivent juste derrière et, en général, il est conseillé
de tester différentes versions pour votre application spécifique : les
résultats peuvent varier.</p>
<p>Ceci est partiellement démontré par le fait que les différentes
couches de BERT encodent des types d'informations très différents, de
sorte que la stratégie de mise en commun appropriée changera en fonction
de l'application parce que les différentes couches encodent des types
d'informations différents.</p>
</div>
<div class="cell markdown" id="m7_CVgejm5pr">
<p><strong>BERT-as-service de Han Xiao</strong></p>
<p>Han Xiao a créé un projet open-source nommé <a
href="https://github.com/hanxiao/bert-as-service">bert-as-service</a>
sur GitHub qui a pour but de créer des embeddings de mots pour votre
texte en utilisant BERT. Han a expérimenté différentes approches pour
combiner ces embeddings, et a partagé quelques conclusions et
discussions sur la <a
href="https://github.com/hanxiao/bert-as-service#speech_balloon-faq">page
FAQ</a> du projet.</p>
<p><code>bert-as-service</code>, par défaut, utilise les résultats de
l'avant-dernière couche du modèle.</p>
<p>On peut résumer le point de vue de Han par ce qui suit :</p>
<ol>
<li><p>Les embeddings commencent dans la première couche sans aucune
information contextuelle (c'est-à-dire que la signification de
l'embedding initial « bank » n'est pas spécifique à la rive ou à la
banque financière).</p></li>
<li><p>Au fur et à mesure que les embeddings se déplacent dans le
réseau, ils acquièrent de plus en plus d'informations contextuelles à
chaque couche.</p></li>
<li><p>Cependant, à mesure qu'on approche de la dernière couche, on
commence à recueillir des informations spécifiques aux tâches de
pré-entraînement de BERT (le « modèle du langage masqué » (MLM) et la «
prédiction de la phrase suivante » (NSP)).</p>
<ul>
<li>Ce que nous voulons, ce sont des embeddings qui encodent bien le
sens du mot...</li>
<li>BERT est motivé pour faire cela, mais il est également motivé pour
encoder tout ce qui pourrait l'aider à déterminer ce qu'est un mot
manquant (MLM), ou si la deuxième phrase vient après la première
(NSP).</li>
</ul></li>
<li><p>L'avant-dernière couche est ce que Han a décidé comme étant un
seuil raisonnable.</p></li>
</ol>
</div>
<section id="4-annexes" class="cell markdown" id="ONLJ36JfPuqf">
<h1>4. Annexes</h1>
</section>
<section id="41-tokens-spéciaux" class="cell markdown"
id="jdw7cLJWMr_Y">
<h2>4.1. Tokens spéciaux</h2>
</section>
<div class="cell markdown" id="Jyx2kQxbnHbM">
<p>Il convient de noter que bien que le <code>[CLS]</code> agisse comme
une « représentation agrégée » pour les tâches de classification, ce
n'est pas le meilleur choix pour un vecteur d'embedding de phrases de
grande qualité. <a
href="https://github.com/google-research/bert/issues/164">Selon</a>
Jacob Devlin, auteur de BERT : «<em>I'm not sure what these vectors are,
since BERT does not generate meaningful sentence vectors. It seems that
this is is doing average pooling over the word tokens to get a sentence
vector, but we never suggested that this will generate meaningful
sentence representations</em>. »</p>
<p>(Toutefois, le token [CLS] devient pertinant si le modèle a été
affiné, lorsque la dernière couche cachée de ce jeton est utilisée comme
« vecteur de phrase » pour la classification des séquences).</p>
</div>
<section id="42-les-mots-hors-vocabulaire" class="cell markdown"
id="EbS8_z6XMuTJ">
<h2>4.2. Les mots hors vocabulaire</h2>
<p>Pour les <strong>mots hors vocabulaire</strong> qui sont composés de
plusieurs embeddings au niveau de la phrase et du caractère, un autre
problème se pose : comment récupérer au mieux cet embedding. La solution
la plus simple consiste à faire la moyenne des embeddings (une solution
sur laquelle s'appuient des modèles d'embeddings similaires avec des
vocabulaires de sous-mots comme fasttext), mais la sommation des
embeddings de sous-mots et la simple prise de l'embedding du dernier
token (notons que les vecteurs sont sensibles au contexte) sont d'autres
stratégies acceptables.</p>
</section>
<section id="43-métriques-de-similarité" class="cell markdown"
id="BokW7CAgMxCB">
<h2>4.3. Métriques de similarité</h2>
<p>Il convient de noter que les <strong>comparaisons de
similarité</strong> au niveau des mots ne sont pas appropriées avec les
embeddings BERT car ces embeddings dépendent du contexte, ce qui
signifie que le vecteur de mot change en fonction de la phrase dans
laquelle il apparaît. Cela permet des choses merveilleuses comme la
polysémie, de sorte que, par exemple, votre représentation encode la
rive « bank » et non une institution financière « bank », mais rend les
comparaisons directes de similarité mot à mot moins valables. Toutefois,
pour les embeddings de phrases, les comparaisons de similarité restent
valables, de sorte qu'il est possible d'interroger, par exemple, une
seule phrase par rapport à un ensemble d'autres phrases afin de trouver
la plus similaire. Selon la métrique de similarité utilisée, les valeurs
de similarité résultantes seront moins informatives que le classement
relatif des résultats de similarité, car de nombreuses métriques de
similarité font des hypothèses sur l'espace vectoriel (dimensions à
pondération égale, par exemple) qui ne sont pas valables pour notre
espace vectoriel à 768 dimensions.</p>
</section>
<section id="44-implémentations" class="cell markdown"
id="0unZ2xh4QDap">
<h2>4.4. Implémentations</h2>
<p>Vous pouvez utiliser ce comme base de votre propre application pour
extraire les caractéristiques BERT du texte. Cependant, des
implémentations très appréciées de <a
href="https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/extract_features.py">pytorch</a>
qui le font pour vous, existent. En outre, <a
href="https://github.com/hanxiao/bert-as-service">bert-as-a-service</a>
est un excellent outil conçu spécifiquement pour exécuter cette tâche
avec de hautes performances, et c'est celui que je recommanderais.
L'auteur a pris grand soin de l'implémentation de l'outil et fournit une
excellente documentation (dont une partie a été utilisée pour aider à
créer ce TP) pour aider les utilisateurs à comprendre les détails plus
nuancés auxquels l'utilisateur est confronté, comme la gestion des
ressources et la stratégie de pooling.</p>
</section>
</body>
</html>
